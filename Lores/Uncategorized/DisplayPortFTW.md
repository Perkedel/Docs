# Display Port FTW

Which is better? DisplayPort? or HDMI?

I've experienced both. **DisplayPort** wins!

Why HDMI Sucks?

- License. HDMI is proprietary expensive! To make one that complies, you must **pay huge fee of license**.
- Quality. As the result, the quality often suffers. A DisplayPort cable that lets you say 4K240, could cost the same as HDMI cable that can so much as 1080p60. Other extra features often times don't work as advertised. You really need to pay exhorbitant fee to get at least minimum viable experience, and yet, people reported the similar issues persisted anyway.

If anyone said *HDMI better*, sponsored or not, that mean they **skipped class way too many times, all intentional**. Don't trust them! **but pls don't go after them personally**, tho.

The problem of DisplayPort therefore only lies so many anymore, which are:

- Niche Adoption. **DisplayPort is 1001% found only on PCs**. Not a single TV, ever, had at least **1**, DisplayPort. All HDMI!! I believe there's mafia shenanigans going on that punished manufacturers who dare to add DP on any of these. Van Elektronische (Perkedel Cinematic Universe only as of 2026) on the other hand is the **world's 1st (& only) manufacturer to have TVs all AV ports are DisplayPort, only**. Because other Van Elektronische devices are also DisplayPort, and they're selling adapters too. If you haven't realized already, the **USB-C interfaced monitor is basically because in DP mode!**

Conversely, you may not know what DisplayPort is capable of, including the ones you believed don't, but you're utterly wrong:

- Audio. It does transmit audio. That's it. Supports multi-channels too.
- USB-C interface. DisplayPort supports this interface. Every Laptops has it, even phones too. But before that existed, you used a **Thunderbolt** shapped (legacy shape) cable for it. Yes, ask OG Mac user! That chamfered square hole on your old Mac is DisplayPort+Thunderbolt!
- Brightness Control. If you set your display mode to SDR, and your monitor supports it, your OS can adjust brightness. **This is not laptop**, it's PC. Especially on KDE Plasma, on Linux, with working GPU driver installed. You'll find a brightness slider there in taskbar. It takes only 1 second delay for the monitor to see the change.
- HDR. Better HDR of course.
  - On Linux it's better. e.g., in KDE Plasma, turning on HDR turns the brightess slider into SDR Brightness. So you'll see brightness of the SDR content, and changes instantly rather than 1 second delay. The slider only affects not-HDR stuffs, and HDR stuffs stays the bright to be. Your monitor often time will be set to maximum brightness it can be, but because the desktop environment has SDR awareness, it won't immediately burn your eyes. You can do this too on Windows, but it's complicated & unintuitive (not aware could've been just that also):
    - Setting
    - Display
    - HDR
    - SDR Content Brightness. I have yet to confirm on Laptop if the Brightness media button adjust SDR Brightness idk.
- Hertz sync. GPU AMD FreeSync or NVidia G-Sync reportedly only works properly in DisplayPort connection. if you still don't understand basically it's *better V-sync*. Instead of the game reducing FPS to your monitor, it's your GPU syncing down to the game. So only when the game refreshed, then it refresh monitor afterward. This helps reduce tearing without hurting FPS in total. idk, my game had no tear and no lag with FreeSync.